diff --git a/run_squad.py b/run_squad.py
index edd4c3e..238a18e 100644
--- a/run_squad.py
+++ b/run_squad.py
@@ -284,7 +284,7 @@ def read_squad_examples(input_file, is_training):
                 doc_tokens[start_position:(end_position + 1)])
             cleaned_answer_text = " ".join(
                 tokenization.whitespace_tokenize(orig_answer_text))
-            if actual_text.find(cleaned_answer_text) == -1:
+            if actual_text.find(cleaned_answer_text) == -1 and actual_text.lower().find(cleaned_answer_text) == -1:
               tf.logging.warning("Could not find answer: '%s' vs. '%s'",
                                  actual_text, cleaned_answer_text)
               continue
@@ -427,31 +427,31 @@ def convert_examples_to_features(examples, tokenizer, max_seq_length,
         start_position = 0
         end_position = 0
 
-      if example_index < 20:
-        tf.logging.info("*** Example ***")
-        tf.logging.info("unique_id: %s" % (unique_id))
-        tf.logging.info("example_index: %s" % (example_index))
-        tf.logging.info("doc_span_index: %s" % (doc_span_index))
-        tf.logging.info("tokens: %s" % " ".join(
-            [tokenization.printable_text(x) for x in tokens]))
-        tf.logging.info("token_to_orig_map: %s" % " ".join(
-            ["%d:%d" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))
-        tf.logging.info("token_is_max_context: %s" % " ".join([
-            "%d:%s" % (x, y) for (x, y) in six.iteritems(token_is_max_context)
-        ]))
-        tf.logging.info("input_ids: %s" % " ".join([str(x) for x in input_ids]))
-        tf.logging.info(
-            "input_mask: %s" % " ".join([str(x) for x in input_mask]))
-        tf.logging.info(
-            "segment_ids: %s" % " ".join([str(x) for x in segment_ids]))
-        if is_training and example.is_impossible:
-          tf.logging.info("impossible example")
-        if is_training and not example.is_impossible:
-          answer_text = " ".join(tokens[start_position:(end_position + 1)])
-          tf.logging.info("start_position: %d" % (start_position))
-          tf.logging.info("end_position: %d" % (end_position))
-          tf.logging.info(
-              "answer: %s" % (tokenization.printable_text(answer_text)))
+      # if example_index < 20:
+      #   tf.logging.info("*** Example ***")
+      #   tf.logging.info("unique_id: %s" % (unique_id))
+      #   tf.logging.info("example_index: %s" % (example_index))
+      #   tf.logging.info("doc_span_index: %s" % (doc_span_index))
+      #   tf.logging.info("tokens: %s" % " ".join(
+      #       [tokenization.printable_text(x) for x in tokens]))
+      #   tf.logging.info("token_to_orig_map: %s" % " ".join(
+      #       ["%d:%d" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))
+      #   tf.logging.info("token_is_max_context: %s" % " ".join([
+      #       "%d:%s" % (x, y) for (x, y) in six.iteritems(token_is_max_context)
+      #   ]))
+      #   tf.logging.info("input_ids: %s" % " ".join([str(x) for x in input_ids]))
+      #   tf.logging.info(
+      #       "input_mask: %s" % " ".join([str(x) for x in input_mask]))
+      #   tf.logging.info(
+      #       "segment_ids: %s" % " ".join([str(x) for x in segment_ids]))
+      #   if is_training and example.is_impossible:
+      #     tf.logging.info("impossible example")
+      #   if is_training and not example.is_impossible:
+      #     answer_text = " ".join(tokens[start_position:(end_position + 1)])
+      #     tf.logging.info("start_position: %d" % (start_position))
+      #     tf.logging.info("end_position: %d" % (end_position))
+      #     tf.logging.info(
+      #         "answer: %s" % (tokenization.printable_text(answer_text)))
 
       feature = InputFeatures(
           unique_id=unique_id,
@@ -657,7 +657,11 @@ def model_fn_builder(bert_config, init_checkpoint, learning_rate,
       start_loss = compute_loss(start_logits, start_positions)
       end_loss = compute_loss(end_logits, end_positions)
 
-      total_loss = (start_loss + end_loss) / 2.0
+      total_loss = tf.div((start_loss + end_loss), 2.0, name='total_loss')
+
+      tf.summary.scalar('loss', total_loss)
+
+      logging_hook = tf.train.LoggingTensorHook({'loss': total_loss}, every_n_iter=10)
 
       train_op = optimization.create_optimizer(
           total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)
@@ -666,7 +670,9 @@ def model_fn_builder(bert_config, init_checkpoint, learning_rate,
           mode=mode,
           loss=total_loss,
           train_op=train_op,
-          scaffold_fn=scaffold_fn)
+          scaffold_fn=scaffold_fn,
+          training_hooks=[logging_hook])
+
     elif mode == tf.estimator.ModeKeys.PREDICT:
       predictions = {
           "unique_ids": unique_ids,
@@ -684,7 +690,7 @@ def model_fn_builder(bert_config, init_checkpoint, learning_rate,
   return model_fn
 
 
-def input_fn_builder(input_file, seq_length, is_training, drop_remainder):
+def input_fn_builder(input_file, seq_length, is_training, drop_remainder, take=None):
   """Creates an `input_fn` closure to be passed to TPUEstimator."""
 
   name_to_features = {
@@ -723,6 +729,9 @@ def input_fn_builder(input_file, seq_length, is_training, drop_remainder):
       d = d.repeat()
       d = d.shuffle(buffer_size=100)
 
+    if take:
+      d = d.take(take)
+
     d = d.apply(
         tf.contrib.data.map_and_batch(
             lambda record: _decode_record(record, name_to_features),
@@ -733,17 +742,113 @@ def input_fn_builder(input_file, seq_length, is_training, drop_remainder):
 
   return input_fn
 
+def should_stop_fn_builder(estimator, val_input_fn, val_examples, val_features, max_steps_without_decrease):
+
+  stats = {
+    'last_stat': 100000,
+    'direction': -1,
+    'steps': 0
+  }
+
+  import re
+  def normalize_answer(s):
+    """Lower text and remove punctuation, articles and extra whitespace."""
+    def remove_articles(text):
+      regex = re.compile(r'\b(a|an|the)\b', re.UNICODE)
+      return re.sub(regex, ' ', text)
+    def white_space_fix(text):
+      return ' '.join(text.split())
+    def remove_punc(text):
+      exclude = set(string.punctuation)
+      return ''.join(ch for ch in text if ch not in exclude)
+    def lower(text):
+      return text.lower()
+    return white_space_fix(remove_articles(remove_punc(lower(s))))
+
+  def get_tokens(s):
+    if not s: return []
+    return normalize_answer(s).split()
+  
+  def compute_exact(a_gold, a_pred):
+    return int(normalize_answer(a_gold) == normalize_answer(a_pred))
+
+  def compute_f1(a_gold, a_pred):
+    gold_toks = get_tokens(a_gold)
+    pred_toks = get_tokens(a_pred)
+    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)
+    num_same = sum(common.values())
+    if len(gold_toks) == 0 or len(pred_toks) == 0:
+      # If either is no-answer, then F1 is 1 if they agree, 0 otherwise
+      return int(gold_toks == pred_toks)
+    if num_same == 0:
+      return 0
+    precision = 1.0 * num_same / len(pred_toks)
+    recall = 1.0 * num_same / len(gold_toks)
+    f1 = (2 * precision * recall) / (precision + recall)
+    return f1
+
+  def update_stats(old_stats, acc_val):
+    diff_val = old_stats['last_stat'] - acc_val
+    if new_dir > 0:
+      # decreasing
+      new_dir = -1
+    elif diff_val < 0:
+      # increasing
+      new_dir = 1
+    else:
+      # constant
+      new_dir = 0
+    
+    return {
+      'last_stat': acc_val,
+      'direction': new_dir,
+      'steps': old_stats['steps'] +1
+    }
+
+  def should_stop():
+    all_results = []
+    for result in estimator.predict(
+        val_input_fn, yield_single_examples=True):
+      unique_id = int(result["unique_ids"])
+      start_logits = [float(x) for x in result["start_logits"].flat]
+      end_logits = [float(x) for x in result["end_logits"].flat]
+      all_results.append(
+          RawResult(
+              unique_id=unique_id,
+              start_logits=start_logits,
+              end_logits=end_logits))
+
+    all_predictions, all_nbest_json, scores_diff_json = create_predictions(
+        val_examples,
+        val_features,
+        all_results,
+        FLAGS.n_best_size,
+        FLAGS.max_answer_length,
+        FLAGS.do_lower_case)
+
+    acc = []
+    for example in val_examples:
+      qid = example.qas_id
+      orig_text = example.orig_text
+      pred_text = all_predictions[qid]
+      exact_score = compute_exact(orig_text, pred_text)
+      f1_score = compute_f1(orig_text, pred_text)
+      acc.append(f1_score)
+
+    acc_val = sum(acc) / len(val_examples)
+    stats = update_stats(stats, acc_val)
+
+    if stats['steps'] >= max_steps_without_decrease:
+      return True
+
+  return should_stop
 
 RawResult = collections.namedtuple("RawResult",
                                    ["unique_id", "start_logits", "end_logits"])
 
 
-def write_predictions(all_examples, all_features, all_results, n_best_size,
-                      max_answer_length, do_lower_case, output_prediction_file,
-                      output_nbest_file, output_null_log_odds_file):
-  """Write final predictions to the json file and log-odds of null if needed."""
-  tf.logging.info("Writing predictions to: %s" % (output_prediction_file))
-  tf.logging.info("Writing nbest to: %s" % (output_nbest_file))
+def create_predictions(all_examples, all_features, all_results, n_best_size,
+                      max_answer_length, do_lower_case):
 
   example_index_to_features = collections.defaultdict(list)
   for feature in all_features:
@@ -903,9 +1008,15 @@ def write_predictions(all_examples, all_features, all_results, n_best_size,
       all_predictions[example.qas_id] = nbest_json[0]["text"]
     else:
       # predict "" iff the null score - the score of best non-null > threshold
-      score_diff = score_null - best_non_null_entry.start_logit - (
-          best_non_null_entry.end_logit)
+      if best_non_null_entry:
+        score_diff = score_null - best_non_null_entry.start_logit - (
+            best_non_null_entry.end_logit)
+      else:
+        # all n best entries are null, we assign a higher diff than threshold
+        score_diff = FLAGS.null_score_diff_threshold + 1.0
+      
       scores_diff_json[example.qas_id] = score_diff
+
       if score_diff > FLAGS.null_score_diff_threshold:
         all_predictions[example.qas_id] = ""
       else:
@@ -913,6 +1024,17 @@ def write_predictions(all_examples, all_features, all_results, n_best_size,
 
     all_nbest_json[example.qas_id] = nbest_json
 
+  return all_predictions, all_nbest_json, scores_diff_json
+
+def write_predictions(all_predictions, all_nbest_json, scores_diff_json,
+                      output_prediction_file, output_nbest_file,
+                      output_null_log_odds_file):
+  
+  """Write final predictions to the json file and log-odds of null if needed."""
+  tf.logging.info("Writing predictions to: %s" % (output_prediction_file))
+  tf.logging.info("Writing nbest to: %s" % (output_nbest_file))
+  tf.logging.info("Writing nbest to: %s" % (output_null_log_odds_file))
+
   with tf.gfile.GFile(output_prediction_file, "w") as writer:
     writer.write(json.dumps(all_predictions, indent=4) + "\n")
 
@@ -1122,6 +1244,108 @@ def validate_flags_or_throw(bert_config):
         "The max_seq_length (%d) must be greater than max_query_length "
         "(%d) + 3" % (FLAGS.max_seq_length, FLAGS.max_query_length))
 
+def prepare_train_input_pipeline(tokenizer, overwrite=False):
+  train_examples = read_squad_examples(
+      input_file=FLAGS.train_file, is_training=True)
+
+  num_train_steps = int(
+      len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)
+
+  num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)
+
+  train_filename = os.path.join(FLAGS.output_dir, "train.tf_record")
+
+  if os.path.exists(train_filename) and not overwrite:
+    return num_train_steps, num_warmup_steps, train_filename
+
+  # Pre-shuffle the input to avoid having to make a very large shuffle
+  # buffer in in the `input_fn`.
+  rng = random.Random(12345)
+  rng.shuffle(train_examples)
+
+  # We write to a temporary file to avoid storing very large constant tensors
+  # in memory.
+  train_writer = FeatureWriter(
+      filename=train_filename,
+      is_training=True)
+  convert_examples_to_features(
+      examples=train_examples,
+      tokenizer=tokenizer,
+      max_seq_length=FLAGS.max_seq_length,
+      doc_stride=FLAGS.doc_stride,
+      max_query_length=FLAGS.max_query_length,
+      is_training=True,
+      output_fn=train_writer.process_feature)
+  train_writer.close()
+  
+  num_train_examples = len(train_examples)
+
+  del train_examples
+
+  return num_train_examples, num_train_steps, num_warmup_steps, train_writer.num_features, train_filename
+
+def prepare_validation_input_pipeline(tokenizer):
+  val_examples = read_squad_examples(
+      input_file=FLAGS.validation_file, is_training=True)
+
+  val_filename = os.path.join(FLAGS.output_dir, "train.tf_record")
+
+  # Pre-shuffle the input to avoid having to make a very large shuffle
+  # buffer in in the `input_fn`.
+  rng = random.Random(12345)
+  rng.shuffle(val_examples)
+
+  # We write to a temporary file to avoid storing very large constant tensors
+  # in memory.
+  val_writer = FeatureWriter(
+      filename=val_filename,
+      is_training=False)
+
+  val_features = []
+
+  def append_feature(feature):
+    val_features.append(feature)
+    val_writer.process_feature(feature)
+
+  # Hack: we pass train=True to get gold answers so we can asses validation accuracy
+  convert_examples_to_features(
+      examples=val_examples,
+      tokenizer=tokenizer,
+      max_seq_length=FLAGS.max_seq_length,
+      doc_stride=FLAGS.doc_stride,
+      max_query_length=FLAGS.max_query_length,
+      is_training=True,
+      output_fn=append_feature)
+  val_writer.close()
+
+  return val_examples, val_features, val_filename
+
+def prepare_evaluate_input_pipeline(tokenizer):
+  eval_examples = read_squad_examples(
+      input_file=FLAGS.predict_file, is_training=False)
+
+  eval_filename = os.path.join(FLAGS.output_dir, "eval.tf_record")
+
+  eval_writer = FeatureWriter(
+      filename=eval_filename,
+      is_training=False)
+  eval_features = []
+
+  def append_feature(feature):
+    eval_features.append(feature)
+    eval_writer.process_feature(feature)
+
+  convert_examples_to_features(
+      examples=eval_examples,
+      tokenizer=tokenizer,
+      max_seq_length=FLAGS.max_seq_length,
+      doc_stride=FLAGS.doc_stride,
+      max_query_length=FLAGS.max_query_length,
+      is_training=False,
+      output_fn=append_feature)
+  eval_writer.close()
+
+  return eval_examples, eval_features, eval_filename
 
 def main(_):
   tf.logging.set_verbosity(tf.logging.INFO)
@@ -1151,20 +1375,12 @@ def main(_):
           num_shards=FLAGS.num_tpu_cores,
           per_host_input_for_training=is_per_host))
 
-  train_examples = None
   num_train_steps = None
-  num_warmup_steps = None
-  if FLAGS.do_train:
-    train_examples = read_squad_examples(
-        input_file=FLAGS.train_file, is_training=True)
-    num_train_steps = int(
-        len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)
-    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)
+  num_warmup_steps  = None
+  train_filename  = None
 
-    # Pre-shuffle the input to avoid having to make a very large shuffle
-    # buffer in in the `input_fn`.
-    rng = random.Random(12345)
-    rng.shuffle(train_examples)
+  if FLAGS.do_train:
+    num_train_examples, num_features, num_train_steps, num_warmup_steps, train_filename = prepare_train_input_pipeline(tokenizer)
 
   model_fn = model_fn_builder(
       bert_config=bert_config,
@@ -1185,67 +1401,64 @@ def main(_):
       predict_batch_size=FLAGS.predict_batch_size)
 
   if FLAGS.do_train:
-    # We write to a temporary file to avoid storing very large constant tensors
-    # in memory.
-    train_writer = FeatureWriter(
-        filename=os.path.join(FLAGS.output_dir, "train.tf_record"),
-        is_training=True)
-    convert_examples_to_features(
-        examples=train_examples,
-        tokenizer=tokenizer,
-        max_seq_length=FLAGS.max_seq_length,
-        doc_stride=FLAGS.doc_stride,
-        max_query_length=FLAGS.max_query_length,
-        is_training=True,
-        output_fn=train_writer.process_feature)
-    train_writer.close()
-
+    
     tf.logging.info("***** Running training *****")
-    tf.logging.info("  Num orig examples = %d", len(train_examples))
-    tf.logging.info("  Num split examples = %d", train_writer.num_features)
+    tf.logging.info("  Num orig examples = %d", num_train_examples)
+    tf.logging.info("  Num split examples = %d", num_features)
     tf.logging.info("  Batch size = %d", FLAGS.train_batch_size)
     tf.logging.info("  Num steps = %d", num_train_steps)
-    del train_examples
 
     train_input_fn = input_fn_builder(
-        input_file=train_writer.filename,
+        input_file=train_filename,
         seq_length=FLAGS.max_seq_length,
         is_training=True,
         drop_remainder=True)
-    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)
+
+    hooks = []
+
+    if FLAGS.validation_file:
+      val_examples, val_features, val_filename = prepare_validation_input_pipeline(tokenizer)
+
+      tf.logging.info("***** Validation data *****")
+      tf.logging.info("  Num orig examples = %d", len(val_examples))
+      tf.logging.info("  Num split examples = %d", len(val_features))
+      tf.logging.info("  Batch size = %d", FLAGS.predict_batch_size)
+
+      # sample 25% of validation set to avoid processing all samples (ideally we should use all)
+      sample_size = (len(val_examples) * 100) // 25
+      # Hack: pass training=True to get data shuffling
+      val_input_fn = input_fn_builder(
+        input_file=val_filename,
+        seq_length=FLAGS.max_seq_length,
+        is_training=True,
+        drop_remainder=True,
+        take=sample_size)
+
+      should_stop_fn = should_stop_fn_builder(estimator, val_input_fn, val_examples,
+                                      val_features, max_steps_without_decrease=10)
+
+      early_stopping = tf.estimator.experimental.make_early_stopping_hook(
+        estimator=estimator,
+        should_stop_fn=should_stop_fn,
+        run_every_secs=None,
+        run_every_steps=100
+      )
+      hooks.append(early_stopping)
+
+    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps, hooks=hooks)
 
   if FLAGS.do_predict:
-    eval_examples = read_squad_examples(
-        input_file=FLAGS.predict_file, is_training=False)
-
-    eval_writer = FeatureWriter(
-        filename=os.path.join(FLAGS.output_dir, "eval.tf_record"),
-        is_training=False)
-    eval_features = []
-
-    def append_feature(feature):
-      eval_features.append(feature)
-      eval_writer.process_feature(feature)
-
-    convert_examples_to_features(
-        examples=eval_examples,
-        tokenizer=tokenizer,
-        max_seq_length=FLAGS.max_seq_length,
-        doc_stride=FLAGS.doc_stride,
-        max_query_length=FLAGS.max_query_length,
-        is_training=False,
-        output_fn=append_feature)
-    eval_writer.close()
+    eval_examples, eval_features, eval_filename = prepare_evaluate_input_pipeline(tokenizer)
 
     tf.logging.info("***** Running predictions *****")
     tf.logging.info("  Num orig examples = %d", len(eval_examples))
     tf.logging.info("  Num split examples = %d", len(eval_features))
     tf.logging.info("  Batch size = %d", FLAGS.predict_batch_size)
-
+    
     all_results = []
 
     predict_input_fn = input_fn_builder(
-        input_file=eval_writer.filename,
+        input_file=eval_filename,
         seq_length=FLAGS.max_seq_length,
         is_training=False,
         drop_remainder=False)
@@ -1270,11 +1483,13 @@ def main(_):
     output_nbest_file = os.path.join(FLAGS.output_dir, "nbest_predictions.json")
     output_null_log_odds_file = os.path.join(FLAGS.output_dir, "null_odds.json")
 
-    write_predictions(eval_examples, eval_features, all_results,
-                      FLAGS.n_best_size, FLAGS.max_answer_length,
-                      FLAGS.do_lower_case, output_prediction_file,
-                      output_nbest_file, output_null_log_odds_file)
+    all_predictions, all_nbest_json, scores_diff_json = create_predictions(eval_examples,
+          eval_features, all_results,
+          FLAGS.n_best_size, FLAGS.max_answer_length,
+          FLAGS.do_lower_case)
 
+    write_predictions(all_predictions, all_nbest_json, scores_diff_json, output_prediction_file,
+                      output_nbest_file, output_null_log_odds_file)
 
 if __name__ == "__main__":
   flags.mark_flag_as_required("vocab_file")
