diff --git a/run_squad.py b/run_squad.py
index edd4c3e..41a4854 100644
--- a/run_squad.py
+++ b/run_squad.py
@@ -284,7 +284,7 @@ def read_squad_examples(input_file, is_training):
                 doc_tokens[start_position:(end_position + 1)])
             cleaned_answer_text = " ".join(
                 tokenization.whitespace_tokenize(orig_answer_text))
-            if actual_text.find(cleaned_answer_text) == -1:
+            if actual_text.find(cleaned_answer_text) == -1 and actual_text.lower().find(cleaned_answer_text) == -1:
               tf.logging.warning("Could not find answer: '%s' vs. '%s'",
                                  actual_text, cleaned_answer_text)
               continue
@@ -427,31 +427,31 @@ def convert_examples_to_features(examples, tokenizer, max_seq_length,
         start_position = 0
         end_position = 0
 
-      if example_index < 20:
-        tf.logging.info("*** Example ***")
-        tf.logging.info("unique_id: %s" % (unique_id))
-        tf.logging.info("example_index: %s" % (example_index))
-        tf.logging.info("doc_span_index: %s" % (doc_span_index))
-        tf.logging.info("tokens: %s" % " ".join(
-            [tokenization.printable_text(x) for x in tokens]))
-        tf.logging.info("token_to_orig_map: %s" % " ".join(
-            ["%d:%d" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))
-        tf.logging.info("token_is_max_context: %s" % " ".join([
-            "%d:%s" % (x, y) for (x, y) in six.iteritems(token_is_max_context)
-        ]))
-        tf.logging.info("input_ids: %s" % " ".join([str(x) for x in input_ids]))
-        tf.logging.info(
-            "input_mask: %s" % " ".join([str(x) for x in input_mask]))
-        tf.logging.info(
-            "segment_ids: %s" % " ".join([str(x) for x in segment_ids]))
-        if is_training and example.is_impossible:
-          tf.logging.info("impossible example")
-        if is_training and not example.is_impossible:
-          answer_text = " ".join(tokens[start_position:(end_position + 1)])
-          tf.logging.info("start_position: %d" % (start_position))
-          tf.logging.info("end_position: %d" % (end_position))
-          tf.logging.info(
-              "answer: %s" % (tokenization.printable_text(answer_text)))
+      # if example_index < 20:
+      #   tf.logging.info("*** Example ***")
+      #   tf.logging.info("unique_id: %s" % (unique_id))
+      #   tf.logging.info("example_index: %s" % (example_index))
+      #   tf.logging.info("doc_span_index: %s" % (doc_span_index))
+      #   tf.logging.info("tokens: %s" % " ".join(
+      #       [tokenization.printable_text(x) for x in tokens]))
+      #   tf.logging.info("token_to_orig_map: %s" % " ".join(
+      #       ["%d:%d" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))
+      #   tf.logging.info("token_is_max_context: %s" % " ".join([
+      #       "%d:%s" % (x, y) for (x, y) in six.iteritems(token_is_max_context)
+      #   ]))
+      #   tf.logging.info("input_ids: %s" % " ".join([str(x) for x in input_ids]))
+      #   tf.logging.info(
+      #       "input_mask: %s" % " ".join([str(x) for x in input_mask]))
+      #   tf.logging.info(
+      #       "segment_ids: %s" % " ".join([str(x) for x in segment_ids]))
+      #   if is_training and example.is_impossible:
+      #     tf.logging.info("impossible example")
+      #   if is_training and not example.is_impossible:
+      #     answer_text = " ".join(tokens[start_position:(end_position + 1)])
+      #     tf.logging.info("start_position: %d" % (start_position))
+      #     tf.logging.info("end_position: %d" % (end_position))
+      #     tf.logging.info(
+      #         "answer: %s" % (tokenization.printable_text(answer_text)))
 
       feature = InputFeatures(
           unique_id=unique_id,
@@ -657,7 +657,7 @@ def model_fn_builder(bert_config, init_checkpoint, learning_rate,
       start_loss = compute_loss(start_logits, start_positions)
       end_loss = compute_loss(end_logits, end_positions)
 
-      total_loss = (start_loss + end_loss) / 2.0
+      total_loss = tf.floordiv((start_loss + end_loss), 2.0, name='total_loss')
 
       train_op = optimization.create_optimizer(
           total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)
@@ -903,9 +903,15 @@ def write_predictions(all_examples, all_features, all_results, n_best_size,
       all_predictions[example.qas_id] = nbest_json[0]["text"]
     else:
       # predict "" iff the null score - the score of best non-null > threshold
-      score_diff = score_null - best_non_null_entry.start_logit - (
-          best_non_null_entry.end_logit)
+      if best_non_null_entry:
+        score_diff = score_null - best_non_null_entry.start_logit - (
+            best_non_null_entry.end_logit)
+      else:
+        # all n best entries are null, we assign a higher diff than threshold
+        score_diff = FLAGS.null_score_diff_threshold + 1.0
+      
       scores_diff_json[example.qas_id] = score_diff
+
       if score_diff > FLAGS.null_score_diff_threshold:
         all_predictions[example.qas_id] = ""
       else:
@@ -1212,7 +1218,14 @@ def main(_):
         seq_length=FLAGS.max_seq_length,
         is_training=True,
         drop_remainder=True)
-    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)
+
+    early_stopping = tf.contrib.estimator.stop_if_no_decrease_hook(
+        estimator,
+        metric_name='total_loss',
+        max_steps_without_decrease=10,
+        min_steps=100)
+
+    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps, hooks=[early_stopping])
 
   if FLAGS.do_predict:
     eval_examples = read_squad_examples(
